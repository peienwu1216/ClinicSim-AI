"""
æ¸¬è©¦å¤šèªè¨€RAGæ”¹é€²æ•ˆæœ
"""

import sys
from pathlib import Path

# æ·»åŠ srcç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.services.rag_service import RAGService

def test_multilingual_queries():
    """æ¸¬è©¦å¤šèªè¨€æŸ¥è©¢ç”Ÿæˆ"""
    print("ğŸ” æ¸¬è©¦å¤šèªè¨€RAGæŸ¥è©¢ç”Ÿæˆ")
    print("=" * 50)
    
    # åˆå§‹åŒ–RAGæœå‹™
    rag_service = RAGService()
    
    # æ¸¬è©¦å°è©±å…§å®¹
    test_conversations = [
        "é†«ç”Ÿï¼Œæˆ‘æƒ³äº†è§£ECGå¿ƒé›»åœ–åœ¨èƒ¸ç—›è¨ºæ–·ä¸­çš„é‡è¦æ€§",
        "How to perform proper history taking for chest pain?",
        "è«‹å•å¦‚ä½•é€²è¡Œèƒ¸ç—›çš„é‘‘åˆ¥è¨ºæ–·ï¼Ÿ",
        "What are the diagnostic criteria for STEMI?",
        "OPQRSTå•è¨ºæŠ€å·§æ˜¯ä»€éº¼ï¼Ÿ"
    ]
    
    for i, conversation in enumerate(test_conversations, 1):
        print(f"\nğŸ“ æ¸¬è©¦å°è©± {i}: {conversation}")
        print("-" * 40)
        
        # ç”ŸæˆæŸ¥è©¢
        queries = rag_service.generate_rag_queries(conversation, "chest_pain")
        
        print("ğŸ” ç”Ÿæˆçš„æŸ¥è©¢:")
        for j, query in enumerate(queries, 1):
            # æª¢æ¸¬èªè¨€
            is_chinese = any('\u4e00' <= char <= '\u9fff' for char in query)
            language = "ä¸­æ–‡" if is_chinese else "è‹±æ–‡"
            print(f"  {j}. [{language}] {query}")
        
        # çµ±è¨ˆèªè¨€åˆ†å¸ƒ
        chinese_count = sum(1 for q in queries if any('\u4e00' <= char <= '\u9fff' for char in q))
        english_count = len(queries) - chinese_count
        print(f"\nğŸ“Š èªè¨€åˆ†å¸ƒ: ä¸­æ–‡ {chinese_count} | è‹±æ–‡ {english_count}")

def test_rag_search_results():
    """æ¸¬è©¦RAGæœå°‹çµæœ"""
    print("\n\nğŸ” æ¸¬è©¦RAGæœå°‹çµæœ")
    print("=" * 50)
    
    rag_service = RAGService()
    
    if not rag_service.is_available():
        print("âŒ RAGæœå‹™ä¸å¯ç”¨ï¼Œè«‹å…ˆåŸ·è¡Œ python build_index.py")
        return
    
    # æ¸¬è©¦ä¸åŒèªè¨€çš„æŸ¥è©¢
    test_queries = [
        "acute chest pain diagnostic protocol",  # è‹±æ–‡æŸ¥è©¢
        "æ€¥æ€§èƒ¸ç—›è¨ºæ–·æµç¨‹",                      # ä¸­æ–‡æŸ¥è©¢
        "ECG interpretation",                   # è‹±æ–‡é†«å­¸è¡“èª
        "å¿ƒé›»åœ–åˆ¤è®€"                            # ä¸­æ–‡é†«å­¸è¡“èª
    ]
    
    for query in test_queries:
        print(f"\nğŸ” æŸ¥è©¢: {query}")
        print("-" * 30)
        
        try:
            # åŸ·è¡Œæœå°‹
            results = rag_service.search(query, k=2)
            
            if results and "æ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™" not in results:
                print("âœ… æ‰¾åˆ°ç›¸é—œè³‡æ–™:")
                # é¡¯ç¤ºçµæœæ‘˜è¦
                lines = results.split('\n---\n')
                for i, line in enumerate(lines[:2], 1):
                    if 'ä¾†æº:' in line:
                        source_line = line.split('\n')[0]
                        content_preview = line.split('\n')[1][:100] + "..." if len(line.split('\n')[1]) > 100 else line.split('\n')[1]
                        print(f"  {i}. {source_line}")
                        print(f"     å…§å®¹é è¦½: {content_preview}")
            else:
                print("âŒ æœªæ‰¾åˆ°ç›¸é—œè³‡æ–™")
                
        except Exception as e:
            print(f"âŒ æœå°‹å¤±æ•—: {e}")

def analyze_document_language_distribution():
    """åˆ†ææ–‡æª”èªè¨€åˆ†å¸ƒ"""
    print("\n\nğŸ“Š åˆ†ææ–‡æª”èªè¨€åˆ†å¸ƒ")
    print("=" * 50)
    
    documents_dir = Path("documents")
    
    if not documents_dir.exists():
        print("âŒ documentsç›®éŒ„ä¸å­˜åœ¨")
        return
    
    # åˆ†ææ–‡æª”èªè¨€åˆ†å¸ƒ
    chinese_docs = []
    english_docs = []
    mixed_docs = []
    
    for file_path in documents_dir.rglob("*.pdf"):
        filename = file_path.name
        
        # ç°¡å–®çš„èªè¨€åˆ¤æ–·ï¼ˆåŸºæ–¼æª”åï¼‰
        has_chinese = any('\u4e00' <= char <= '\u9fff' for char in filename)
        has_english = any(char.isalpha() and ord(char) < 128 for char in filename)
        
        if has_chinese and has_english:
            mixed_docs.append(filename)
        elif has_chinese:
            chinese_docs.append(filename)
        elif has_english:
            english_docs.append(filename)
    
    print(f"ğŸ“„ æ–‡æª”èªè¨€åˆ†å¸ƒ:")
    print(f"  ä¸­æ–‡æ–‡æª”: {len(chinese_docs)}")
    for doc in chinese_docs:
        print(f"    - {doc}")
    
    print(f"\n  è‹±æ–‡æ–‡æª”: {len(english_docs)}")
    for doc in english_docs:
        print(f"    - {doc}")
    
    print(f"\n  æ··åˆèªè¨€: {len(mixed_docs)}")
    for doc in mixed_docs:
        print(f"    - {doc}")
    
    print(f"\nğŸ“Š ç¸½è¨ˆ: {len(chinese_docs) + len(english_docs) + len(mixed_docs)} å€‹æ–‡æª”")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ§ª å¤šèªè¨€RAGæ”¹é€²æ¸¬è©¦")
    print("=" * 60)
    
    # æ¸¬è©¦æŸ¥è©¢ç”Ÿæˆ
    test_multilingual_queries()
    
    # æ¸¬è©¦æœå°‹çµæœ
    test_rag_search_results()
    
    # åˆ†ææ–‡æª”åˆ†å¸ƒ
    analyze_document_language_distribution()
    
    print("\n\nğŸ‰ æ¸¬è©¦å®Œæˆï¼")
    print("\nğŸ’¡ æ”¹é€²å»ºè­°:")
    print("1. å¤šèªè¨€æŸ¥è©¢ç¢ºä¿èƒ½æœå°‹åˆ°ä¸­è‹±æ–‡è³‡æ–™")
    print("2. é†«å­¸è¡“èªæŸ¥è©¢æ›´å®¹æ˜“åŒ¹é…åœ‹éš›æŒ‡å—")
    print("3. æ ¹æ“šå°è©±å…§å®¹å‹•æ…‹èª¿æ•´æŸ¥è©¢å„ªå…ˆé †åº")
    print("4. å»ºè­°å¢åŠ æ›´å¤šè‹±æ–‡é†«å­¸æ–‡æª”ä»¥å¹³è¡¡èªè¨€åˆ†å¸ƒ")

if __name__ == "__main__":
    main()
