import os
import requests
import base64
from pathlib import Path
from langchain_community.document_loaders import PyMuPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

# --- è¨­å®š ---
DOCUMENTS_PATH = "documents"  # å°‡ä½ çš„ PDFã€TXT æª”æ¡ˆæ”¾åœ¨é€™è£¡
INDEX_PATH = "faiss_index"    # å‘é‡è³‡æ–™åº«å„²å­˜è·¯å¾‘
EMBEDDING_MODEL = "nomic-ai/nomic-embed-text-v1.5" # ä¸­è‹±é›™èªçš†è¡¨ç¾å„ªç•°çš„é–‹æºæ¨¡å‹

# --- Lemonade Server è¨­å®š ---
LEMONADE_API_URL = "http://127.0.0.1:8080/v1/chat/completions"
LEMONADE_API_KEY = "lemonade"  # API Key èªè­‰
LEMONADE_VLM_MODEL = "squeeze-ai-lab/TinyAgent-1.1B"   # å¤šæ¨¡æ…‹æ¨¡å‹

def process_images_to_documents(images_path: Path) -> list[Document]:
    """
    ä½¿ç”¨ Lemonade Server çš„å¤šæ¨¡æ…‹æ¨¡å‹è™•ç†åœ–åƒï¼Œå°‡å…¶è½‰æ›ç‚ºæ–‡å­—æè¿°ä¸¦å»ºç«‹ Document ç‰©ä»¶ã€‚
    
    Args:
        images_path: åœ–åƒæª”æ¡ˆæ‰€åœ¨çš„è·¯å¾‘
        
    Returns:
        list[Document]: åŒ…å«åœ–åƒæè¿°çš„ Document ç‰©ä»¶åˆ—è¡¨
    """
    if not images_path.exists():
        print(f"âš ï¸ è­¦å‘Šï¼šåœ–åƒè·¯å¾‘ '{images_path}' ä¸å­˜åœ¨ã€‚")
        return []
    
    # å¯¼å…¥ call_AI.py çš„æ–¹æ³•
    from call_AI import call_ai_multimodal
    
    image_documents = []
    supported_formats = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}
    
    print(f"ğŸ” é–‹å§‹è™•ç†åœ–åƒæª”æ¡ˆ...")
    
    for image_file in images_path.glob('**/*'):
        if image_file.is_file() and image_file.suffix.lower() in supported_formats:
            try:
                print(f"ğŸ“· æ­£åœ¨è™•ç†åœ–åƒ: {image_file.name}")
                
                # 1. è®€å–ä¸¦ç·¨ç¢¼åœ–åƒ
                with open(image_file, 'rb') as f:
                    image_data = f.read()
                image_base64 = base64.b64encode(image_data).decode('utf-8')
                
                # 2. æº–å‚™å¤šæ¨¡æ€æ¶ˆæ¯
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "è«‹è©³ç´°æè¿°é€™å¼µåœ–ç‰‡ä¸­çš„å…§å®¹ï¼ŒåŒ…æ‹¬æ‰€æœ‰å¯è¦‹çš„é†«å­¸ç›¸é—œè³‡è¨Šã€åœ–è¡¨ã€æ–‡å­—ç­‰ã€‚"
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/{image_file.suffix[1:]};base64,{image_base64}"
                                }
                            }
                        ]
                    }
                ]
                
                # 3. ä½¿ç”¨ call_AI.py çš„å¤šæ¨¡æ€æ–¹æ³•
                description = call_ai_multimodal(messages, LEMONADE_VLM_MODEL)
                
                # å»ºç«‹ Document ç‰©ä»¶
                doc = Document(
                    page_content=description,
                    metadata={
                        "source": str(image_file),
                        "type": "image",
                        "filename": image_file.name,
                        "description_length": len(description)
                    }
                )
                image_documents.append(doc)
                print(f"âœ… åœ–åƒè™•ç†å®Œæˆ: {image_file.name}")
                    
            except Exception as e:
                print(f"âŒ è™•ç†åœ–åƒ {image_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
    
    print(f"ğŸ¯ åœ–åƒè™•ç†å®Œæˆï¼Œå…±è™•ç† {len(image_documents)} å¼µåœ–ç‰‡ã€‚")
    return image_documents

def build_index():
    """
    è®€å– documents è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼Œå°‡å…¶è½‰æ›ç‚ºå‘é‡ä¸¦å»ºç«‹ FAISS ç´¢å¼•ã€‚
    é€™æ˜¯ä¸€å€‹ä¸€æ¬¡æ€§çš„éç¨‹ï¼Œåœ¨é–‹ç™¼å‰é‹è¡Œå³å¯ã€‚
    æ”¯æ´çš„æ–‡ä»¶é¡å‹ï¼šPDFã€TXTã€MDã€ä»¥åŠå„ç¨®åœ–åƒæ ¼å¼ã€‚
    """
    print("--- é–‹å§‹å»ºç«‹ RAG å‘é‡ç´¢å¼• ---")
    
    if not os.path.exists(DOCUMENTS_PATH):
        os.makedirs(DOCUMENTS_PATH)
        print(f"å·²å»ºç«‹ '{DOCUMENTS_PATH}' è³‡æ–™å¤¾ï¼Œè«‹å°‡ä½ çš„ PDFã€TXT æ–‡ä»¶æ”¾å…¥å…¶ä¸­å¾Œå†åŸ·è¡Œä¸€æ¬¡ã€‚")
        return

    # 1. è¼‰å…¥æ‰€æœ‰æ–‡ä»¶ï¼ˆæ–‡å­—æ–‡ä»¶ï¼‰
    all_docs = []
    p = Path(DOCUMENTS_PATH)
    for file in p.glob('**/*'):
        if file.is_dir():
            continue
        try:
            if file.suffix == '.pdf':
                print(f"æ­£åœ¨è¼‰å…¥ PDF: {file.name}")
                loader = PyMuPDFLoader(str(file))
                all_docs.extend(loader.load())
            elif file.suffix in ['.txt', '.md']:
                print(f"æ­£åœ¨è¼‰å…¥æ–‡å­—æª”: {file.name}")
                loader = TextLoader(str(file), encoding='utf-8')
                all_docs.extend(loader.load())
        except Exception as e:
            print(f"è¼‰å…¥æª”æ¡ˆ {file.name} å¤±æ•—: {e}")

    # 2. è™•ç†åœ–åƒæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    images_path = Path(DOCUMENTS_PATH) / "images"
    if images_path.exists():
        print("ğŸ–¼ï¸ ç™¼ç¾åœ–åƒè³‡æ–™å¤¾ï¼Œé–‹å§‹è™•ç†åœ–åƒ...")
        image_docs = process_images_to_documents(images_path)
        all_docs.extend(image_docs)
    else:
        print("â„¹ï¸ æœªç™¼ç¾ 'images' å­è³‡æ–™å¤¾ï¼Œè·³éåœ–åƒè™•ç†ã€‚")

    if not all_docs:
        print(f"åœ¨ '{DOCUMENTS_PATH}' è³‡æ–™å¤¾ä¸­æ‰¾ä¸åˆ°ä»»ä½•å¯è®€å–çš„æ–‡ä»¶ã€‚")
        return
        
    print(f"æ–‡ä»¶è¼‰å…¥å®Œæˆï¼Œå…± {len(all_docs)} é ã€‚é–‹å§‹åˆ‡å¡Š...")
    
    # 3. åˆ‡å‰²æ–‡ä»¶
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    chunks = text_splitter.split_documents(all_docs)
    print(f"åˆ‡å¡Šå®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} å€‹çŸ¥è­˜ç‰‡æ®µã€‚")

    # 4. åˆå§‹åŒ– Embedding æ¨¡å‹
    print(f"æ­£åœ¨åˆå§‹åŒ– Embedding æ¨¡å‹: {EMBEDDING_MODEL} (å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“ä¸‹è¼‰)...")
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'trust_remote_code': True},
        encode_kwargs={'normalize_embeddings': True}
    )

    # 5. å»ºç«‹ FAISS ç´¢å¼•ä¸¦å„²å­˜
    print("æ­£åœ¨å°‡çŸ¥è­˜ç‰‡æ®µè½‰æ›ç‚ºå‘é‡ä¸¦å»ºç«‹ FAISS ç´¢å¼•...")
    vectorstore = FAISS.from_documents(chunks, embeddings)
    vectorstore.save_local(INDEX_PATH)
    
    print("\n--- âœ… RAG ç´¢å¼•å»ºç«‹æˆåŠŸï¼---")
    print(f"ç´¢å¼•æª”æ¡ˆå·²å„²å­˜è‡³ '{INDEX_PATH}' è³‡æ–™å¤¾ã€‚")
    print("é‡è¦ï¼šè«‹ç¢ºä¿ä½ çš„ .gitignore æª”æ¡ˆä¸­æœ‰ `faiss_index/` é€™ä¸€è¡Œã€‚")

if __name__ == '__main__':
    build_index()
