"""
RAG (Retrieval-Augmented Generation) æœå‹™
"""

import os
from typing import List, Dict, Any, Optional
from pathlib import Path

from ..config.settings import get_settings
from ..models.report import Citation


class RAGService:
    """RAG æœå‹™é¡"""
    
    def __init__(self, settings=None):
        self.settings = settings or get_settings()
        self.vector_store = None
        self.embeddings = None
        self._initialize_rag()
    
    def _initialize_rag(self) -> None:
        """åˆå§‹åŒ– RAG ç³»çµ±"""
        if not self.settings.faiss_index_dir.exists():
            print(f"âš ï¸ è­¦å‘Šï¼šRAG ç´¢å¼•ç›®éŒ„ä¸å­˜åœ¨: {self.settings.faiss_index_dir}")
            print("   è«‹å…ˆåŸ·è¡Œ `python build_index.py` ä¾†å»ºç«‹ç´¢å¼•")
            return
        
        try:
            print("ğŸ’¡ [RAG] æ­£åœ¨è¼‰å…¥ RAG å‘é‡ç´¢å¼•...")
            
            # åˆå§‹åŒ– embedding æ¨¡å‹
            from langchain_community.embeddings import HuggingFaceEmbeddings
            self.embeddings = HuggingFaceEmbeddings(
                model_name=self.settings.rag_model_name,
                model_kwargs={'trust_remote_code': True},
                encode_kwargs={'normalize_embeddings': True}
            )
            
            # è¼‰å…¥ FAISS ç´¢å¼•
            from langchain_community.vectorstores import FAISS
            self.vector_store = FAISS.load_local(
                str(self.settings.faiss_index_dir),
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            
            print("âœ… [RAG] RAG ç´¢å¼•è¼‰å…¥æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ [RAG] RAG ç´¢å¼•è¼‰å…¥å¤±æ•—: {e}")
            self.vector_store = None
            self.embeddings = None
    
    def search(self, query: str, k: Optional[int] = None) -> str:
        """åŸ·è¡Œ RAG æœå°‹"""
        if not self.vector_store:
            return "RAG ç³»çµ±æœªåˆå§‹åŒ–ï¼Œç„¡æ³•åŸ·è¡Œæœå°‹ã€‚"
        
        k = k or self.settings.rag_search_k
        print(f"[RAG] æ­£åœ¨æœå°‹é—œæ–¼ '{query}' çš„è³‡æ–™...")
        
        try:
            # ä½¿ç”¨ç›¸ä¼¼åº¦æœå°‹ï¼Œä¸¦ç²å–åˆ†æ•¸
            results_with_scores = self.vector_store.similarity_search_with_score(query, k=k*2)  # ç²å–æ›´å¤šçµæœä»¥ä¾¿éæ¿¾
            
            if not results_with_scores:
                return "åœ¨çŸ¥è­˜åº«ä¸­æ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ã€‚"
            
            # éæ¿¾ä½ç›¸é—œæ€§çš„çµæœ
            filtered_results = []
            for doc, score in results_with_scores:
                # ç›¸ä¼¼åº¦åˆ†æ•¸è¶Šå°è¡¨ç¤ºè¶Šç›¸ä¼¼ï¼Œè¨­å®šæ›´åš´æ ¼çš„é–¾å€¼
                if score < 1.2:  # æ›´åš´æ ¼çš„é–¾å€¼ä»¥ç¢ºä¿ç›¸é—œæ€§
                    # é¡å¤–æª¢æŸ¥å…§å®¹æ˜¯å¦èˆ‡æŸ¥è©¢ç›¸é—œ
                    if self._is_content_relevant(doc.page_content, query):
                        filtered_results.append(doc)
                if len(filtered_results) >= k:  # é™åˆ¶çµæœæ•¸é‡
                    break
            
            if not filtered_results:
                # å¦‚æœéæ¿¾å¾Œæ²’æœ‰çµæœï¼Œä½¿ç”¨å‰kå€‹çµæœï¼Œä½†å†æ¬¡æª¢æŸ¥ç›¸é—œæ€§
                for doc, score in results_with_scores[:k]:
                    if self._is_content_relevant(doc.page_content, query):
                        filtered_results.append(doc)
                        if len(filtered_results) >= k:
                            break
                
                # å¦‚æœé‚„æ˜¯æ²’æœ‰ç›¸é—œçµæœï¼Œè¿”å›ç©º
                if not filtered_results:
                    return "åœ¨çŸ¥è­˜åº«ä¸­æ‰¾ä¸åˆ°èˆ‡æŸ¥è©¢ç›¸é—œçš„è³‡æ–™ã€‚"
            
            # æ ¼å¼åŒ–çµæœï¼Œç¢ºä¿å…§å®¹èˆ‡æŸ¥è©¢ç›¸é—œ
            context = "\n---\n".join([
                self._format_document_content_with_query(doc.metadata.get('source', 'æœªçŸ¥'), doc.page_content, query)
                for doc in filtered_results
            ])
            
            return context
            
        except Exception as e:
            print(f"[RAG] æœå°‹å¤±æ•—: {e}")
            return f"RAG æœå°‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
    
    def _format_document_content(self, source: str, content: str) -> str:
        """æ ¼å¼åŒ–æ–‡æª”å…§å®¹ï¼Œç§»é™¤ä¸å¿…è¦çš„æ¨™é¡Œå’Œæ ¼å¼"""
        # ç§»é™¤æ–‡æª”é–‹é ­çš„æ¨™é¡Œï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€è¡Œæˆ–å‰å¹¾è¡Œï¼‰
        lines = content.strip().split('\n')
        
        # è·³éé–‹é ­çš„æ¨™é¡Œè¡Œï¼ˆé€šå¸¸æ˜¯æ–‡æª”æ¨™é¡Œï¼‰
        content_lines = []
        skip_title = True
        
        for line in lines:
            line = line.strip()
            
            # å¦‚æœæ˜¯ç©ºè¡Œï¼Œè·³é
            if not line:
                continue
                
            # å¦‚æœé‡åˆ°Markdownæ¨™é¡Œï¼ˆ## æˆ– ###ï¼‰ï¼Œç§»é™¤æ¨™é¡Œæ¨™è¨˜ä½†ä¿ç•™å…§å®¹
            if line.startswith('##') or line.startswith('###'):
                if skip_title:
                    continue
                else:
                    # ç§»é™¤æ¨™é¡Œæ¨™è¨˜ï¼Œä¿ç•™å…§å®¹
                    line = line.lstrip('#').strip()
            
            # è·³éæ–‡æª”é–‹é ­çš„æ¨™é¡Œï¼ˆåŒ…å«ç‰¹å®šé—œéµè©çš„çŸ­è¡Œï¼‰
            if skip_title and len(line) < 50 and ('æŒ‡å¼•' in line or 'æŒ‡å—' in line or 'è‡¨åºŠ' in line):
                skip_title = False
                continue
            
            # ç§»é™¤Markdownæ ¼å¼æ¨™è¨˜
            line = line.replace('**', '').replace('*', '')
            
            skip_title = False
            if line:  # åªæ·»åŠ éç©ºè¡Œ
                content_lines.append(line)
        
        # é‡æ–°çµ„åˆå…§å®¹ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼Œè®“å…§å®¹æ›´è‡ªç„¶
        formatted_content = ' '.join(content_lines)
        
        # æ¸…ç†å¤šé¤˜çš„ç©ºæ ¼
        import re
        formatted_content = re.sub(r'\s+', ' ', formatted_content).strip()
        
        # é™åˆ¶å…§å®¹é•·åº¦ï¼Œé¿å…éé•·çš„å¼•ç”¨
        if len(formatted_content) > 600:
            formatted_content = formatted_content[:600] + "..."
        
        # ç¾åŒ–æª”åé¡¯ç¤º
        filename = source.split('/')[-1]
        clean_name = filename.replace('.pdf', '').replace('.txt', '').replace('.jpg', '').replace('_', ' ')
        return f"ğŸ“š **{clean_name}**\n\n{formatted_content}"
    
    def _format_document_content_with_query(self, source: str, content: str, query: str) -> str:
        """æ ¼å¼åŒ–æ–‡æª”å…§å®¹ï¼Œç¢ºä¿èˆ‡æŸ¥è©¢ç›¸é—œï¼Œä¸¦æå–æœ€ç›¸é—œçš„æ®µè½"""
        # ç§»é™¤æ–‡æª”é–‹é ­çš„æ¨™é¡Œï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€è¡Œæˆ–å‰å¹¾è¡Œï¼‰
        lines = content.strip().split('\n')
        
        # è·³éé–‹é ­çš„æ¨™é¡Œè¡Œï¼ˆé€šå¸¸æ˜¯æ–‡æª”æ¨™é¡Œï¼‰
        content_lines = []
        skip_title = True
        
        for line in lines:
            line = line.strip()
            
            # å¦‚æœæ˜¯ç©ºè¡Œï¼Œè·³é
            if not line:
                continue
                
            # å¦‚æœé‡åˆ°Markdownæ¨™é¡Œï¼ˆ## æˆ– ###ï¼‰ï¼Œç§»é™¤æ¨™é¡Œæ¨™è¨˜ä½†ä¿ç•™å…§å®¹
            if line.startswith('##') or line.startswith('###'):
                if skip_title:
                    continue
                else:
                    # ç§»é™¤æ¨™é¡Œæ¨™è¨˜ï¼Œä¿ç•™å…§å®¹
                    line = line.lstrip('#').strip()
            
            # è·³éæ–‡æª”é–‹é ­çš„æ¨™é¡Œï¼ˆåŒ…å«ç‰¹å®šé—œéµè©çš„çŸ­è¡Œï¼‰
            if skip_title and len(line) < 50 and ('æŒ‡å¼•' in line or 'æŒ‡å—' in line or 'è‡¨åºŠ' in line):
                skip_title = False
                continue
            
            # ç§»é™¤Markdownæ ¼å¼æ¨™è¨˜
            line = line.replace('**', '').replace('*', '')
            
            skip_title = False
            if line:  # åªæ·»åŠ éç©ºè¡Œ
                content_lines.append(line)
        
        # é‡æ–°çµ„åˆå…§å®¹
        full_content = ' '.join(content_lines)
        
        # æ ¹æ“šæŸ¥è©¢æå–æœ€ç›¸é—œçš„æ®µè½
        relevant_content = self._extract_relevant_paragraph(full_content, query)
        
        # æ¸…ç†å¤šé¤˜çš„ç©ºæ ¼
        import re
        relevant_content = re.sub(r'\s+', ' ', relevant_content).strip()
        
        # é™åˆ¶å…§å®¹é•·åº¦ï¼Œé¿å…éé•·çš„å¼•ç”¨
        if len(relevant_content) > 500:  # æ¸›å°‘é•·åº¦ä»¥æé«˜ç²¾æº–åº¦
            relevant_content = relevant_content[:500] + "..."
        
        # ç¾åŒ–æª”åé¡¯ç¤º
        filename = source.split('/')[-1]
        clean_name = filename.replace('.pdf', '').replace('.txt', '').replace('.jpg', '').replace('_', ' ')
        return f"ğŸ“š **{clean_name}**\n\n{relevant_content}"
    
    def _extract_relevant_paragraph(self, content: str, query: str) -> str:
        """å¾å…§å®¹ä¸­æå–èˆ‡æŸ¥è©¢æœ€ç›¸é—œçš„æ®µè½"""
        # å°‡å…§å®¹æŒ‰å¥è™Ÿåˆ†å‰²æˆæ®µè½
        sentences = content.split('ã€‚')
        
        # è¨ˆç®—æ¯å€‹å¥å­èˆ‡æŸ¥è©¢çš„ç›¸é—œæ€§
        query_words = set(query.lower().split())
        best_sentence = ""
        best_score = 0
        
        # å®šç¾©æŸ¥è©¢ç›¸é—œçš„é—œéµè©æ˜ å°„
        query_keywords = {
            'ecg': ['ecg', 'å¿ƒé›»åœ–', '12å°ç¨‹', 'å¿ƒé›»'],
            'opqrst': ['opqrst', 'å•è¨º', 'ç—…å²', 'onset', 'quality', 'radiation', 'severity', 'time'],
            'æ€¥æ€§å† å¿ƒç—‡': ['æ€¥æ€§å† å¿ƒç—‡', 'acs', 'å¿ƒè‚Œæ¢—å¡', 'å¿ƒçµç—›'],
            'å¿ƒè‚Œéˆ£è›‹ç™½': ['troponin', 'å¿ƒè‚Œéˆ£è›‹ç™½', 'å¿ƒè‚Œé…µç´ ', 'ck-mb', 'æª¢é©—', 'æŠ½è¡€']
        }
        
        # æ“´å±•æŸ¥è©¢é—œéµè©
        expanded_query_words = query_words.copy()
        for key, keywords in query_keywords.items():
            if any(word in query.lower() for word in [key] + keywords):
                expanded_query_words.update(keywords)
        
        for sentence in sentences:
            if not sentence.strip():
                continue
                
            sentence_words = set(sentence.lower().split())
            
            # è¨ˆç®—è©å½™é‡ç–Šåº¦
            overlap = len(expanded_query_words.intersection(sentence_words))
            if overlap > 0:
                # è¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸ï¼ˆé‡ç–Šè©æ•¸ / æŸ¥è©¢è©æ•¸ï¼‰
                score = overlap / len(expanded_query_words)
                if score > best_score:
                    best_score = score
                    best_sentence = sentence.strip()
        
        # å¦‚æœæ‰¾åˆ°ç›¸é—œå¥å­ï¼Œè¿”å›è©²å¥å­åŠå…¶å‰å¾Œæ–‡
        if best_sentence and best_score > 0.1:  # è¨­å®šæœ€å°ç›¸é—œæ€§é–¾å€¼
            # æ‰¾åˆ°æœ€ä½³å¥å­åœ¨åŸæ–‡ä¸­çš„ä½ç½®
            best_index = -1
            for i, sent in enumerate(sentences):
                if sent.strip() == best_sentence:
                    best_index = i
                    break
            
            if best_index >= 0:
                # è¿”å›æœ€ä½³å¥å­åŠå…¶å‰å¾Œå„ä¸€å¥
                start = max(0, best_index - 1)
                end = min(len(sentences), best_index + 2)
                relevant_sentences = sentences[start:end]
                return 'ã€‚'.join(relevant_sentences)
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°è¶³å¤ ç›¸é—œçš„å¥å­ï¼Œå˜—è©¦åœ¨æ•´å€‹å…§å®¹ä¸­æœå°‹é—œéµè©
        if best_score <= 0.1:
            # ç›´æ¥åœ¨å…§å®¹ä¸­æœå°‹é—œéµè©ï¼Œè¿”å›åŒ…å«é—œéµè©çš„æ®µè½
            for keyword in expanded_query_words:
                if keyword in content.lower():
                    # æ‰¾åˆ°é—œéµè©çš„ä½ç½®
                    keyword_pos = content.lower().find(keyword)
                    if keyword_pos >= 0:
                        # æå–é—œéµè©å‰å¾Œå„200å€‹å­—ç¬¦
                        start = max(0, keyword_pos - 200)
                        end = min(len(content), keyword_pos + len(keyword) + 200)
                        relevant_content = content[start:end]
                        return relevant_content
        
        # å¦‚æœéƒ½æ²’æœ‰æ‰¾åˆ°ï¼Œè¿”å›å‰å¹¾å¥
        return 'ã€‚'.join(sentences[:2]) if len(sentences) >= 2 else content
    
    def _is_content_relevant(self, content: str, query: str) -> bool:
        """æª¢æŸ¥å…§å®¹æ˜¯å¦èˆ‡æŸ¥è©¢ç›¸é—œ"""
        content_lower = content.lower()
        query_lower = query.lower()
        
        # å®šç¾©æŸ¥è©¢ç›¸é—œçš„é—œéµè©æ˜ å°„
        query_keywords = {
            'ecg': ['ecg', 'å¿ƒé›»åœ–', '12å°ç¨‹', 'å¿ƒé›»', 'electrocardiogram'],
            'opqrst': ['opqrst', 'å•è¨º', 'ç—…å²', 'onset', 'quality', 'radiation', 'severity', 'time', 'ç™¼ä½œ', 'æ€§è³ª', 'æ”¾å°„', 'åš´é‡', 'æ™‚é–“'],
            'æ€¥æ€§å† å¿ƒç—‡': ['æ€¥æ€§å† å¿ƒç—‡', 'acs', 'å¿ƒè‚Œæ¢—å¡', 'å¿ƒçµç—›', 'acute coronary syndrome', 'myocardial infarction'],
            'å¿ƒè‚Œéˆ£è›‹ç™½': ['troponin', 'å¿ƒè‚Œéˆ£è›‹ç™½', 'å¿ƒè‚Œé…µç´ ', 'ck-mb', 'æª¢é©—', 'æŠ½è¡€', 'cardiac troponin'],
            'å¯¦é©—å®¤æª¢æŸ¥': ['å¯¦é©—å®¤', 'æª¢é©—', 'è¡€æ¶²', 'laboratory', 'test', 'blood test'],
            'è¡€æ¶²æª¢é©—': ['è¡€æ¶²', 'æª¢é©—', 'æŠ½è¡€', 'blood', 'test', 'laboratory']
        }
        
        # æª¢æŸ¥ç›´æ¥åŒ¹é…
        if any(word in content_lower for word in query_lower.split()):
            return True
        
        # æª¢æŸ¥é—œéµè©æ˜ å°„
        for key, keywords in query_keywords.items():
            if any(word in query_lower for word in [key] + keywords):
                if any(keyword in content_lower for keyword in keywords):
                    return True
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«ä¸ç›¸é—œçš„å…§å®¹ï¼ˆå¦‚åœ–ç‰‡OCRçµæœï¼‰
        irrelevant_indicators = ['æ¡ˆä¾‹', 'æ¡ˆä¾‹2', 'æ¡ˆä¾‹3', 'æ¡ˆä¾‹4', 'æ¡ˆä¾‹5', 'ç‰¹é›·å¼—', 'é›…å„å¸ƒå…ˆç”Ÿ']
        if any(indicator in content for indicator in irrelevant_indicators):
            return False
        
        # æª¢æŸ¥å…§å®¹é•·åº¦ï¼Œå¤ªçŸ­çš„å…§å®¹å¯èƒ½ä¸ç›¸é—œ
        if len(content.strip()) < 20:
            return False
        
        return True
    
    def search_with_citations(self, queries: List[str], k: Optional[int] = None) -> List[Citation]:
        """åŸ·è¡Œå¤šå€‹æŸ¥è©¢ä¸¦è¿”å›å¸¶å¼•è¨»çš„çµæœ"""
        citations = []
        
        for i, query in enumerate(queries, 1):
            context = self.search(query, k)
            
            if context and "RAG ç³»çµ±æœªåˆå§‹åŒ–" not in context and "æ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™" not in context:
                citation = Citation(
                    id=i,
                    query=query,
                    source=f"è‡¨åºŠæŒ‡å¼• {i}",
                    content=context,
                    metadata={"search_k": k or self.settings.rag_search_k}
                )
                citations.append(citation)
        
        return citations
    
    def generate_rag_queries(self, conversation_text: str, case_type: str = "chest_pain") -> List[str]:
        """æ ¹æ“šå°è©±å…§å®¹å’Œæ¡ˆä¾‹é¡å‹ç”Ÿæˆ RAG æŸ¥è©¢"""
        base_queries = {
            "chest_pain": [
                "æ€¥æ€§èƒ¸ç—›è¨ºæ–·æµç¨‹å’Œæª¢æŸ¥é †åº",
                "ECG å¿ƒé›»åœ–åœ¨èƒ¸ç—›è©•ä¼°ä¸­çš„é‡è¦æ€§",
                "STEMI å’Œä¸ç©©å®šå‹å¿ƒçµç—›çš„è¨ºæ–·æ¨™æº–",
                "èƒ¸ç—›å•è¨ºçš„ OPQRST æŠ€å·§å’Œé‡é»"
            ],
            "default": [
                "è‡¨åºŠè¨ºæ–·æµç¨‹å’Œæª¢æŸ¥é †åº",
                "é—œéµç—‡ç‹€çš„è©•ä¼°æ–¹æ³•",
                "è¨ºæ–·æ¨™æº–å’Œæ²»ç™‚æŒ‡å¼•",
                "å•è¨ºæŠ€å·§å’Œé‡é»"
            ]
        }
        
        return base_queries.get(case_type, base_queries["default"])
    
    def is_available(self) -> bool:
        """æª¢æŸ¥ RAG æœå‹™æ˜¯å¦å¯ç”¨"""
        return self.vector_store is not None
    
    def get_index_info(self) -> Dict[str, Any]:
        """å–å¾—ç´¢å¼•è³‡è¨Š"""
        if not self.vector_store:
            return {"status": "not_initialized", "index_path": str(self.settings.faiss_index_dir)}
        
        return {
            "status": "initialized",
            "index_path": str(self.settings.faiss_index_dir),
            "embedding_model": self.settings.rag_model_name,
            "search_k": self.settings.rag_search_k
        }
